# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words)
scores.df = data.frame(score=scores, text=sentences, size=seq(length(scores)))
return(scores.df)
}
sentimentalanalysis<-function(entity1text,entity1entry){
# A compiled list of words expressing positive and negative sentiments ----
#http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html
# List of words and additional information on the original source from Jeffrey Breen's github site at:
#https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English
positivewords=readLines("positive_words.txt")
negativewords=readLines("negative_words.txt")
#Applying score.sentiment algorithm to cleaned tweets and getting data frames of tweets, net sentiment score for a tweet
#(number of positive sentiments minus negative sentiments)
entity1score = score.sentiment(CleanTweets(entity1text),positivewords,negativewords)
# Adding a dummy variable useful for a ggplot
entity1score$entity = entity1entry
entityscores<-data.frame()
#combine all of this
entityscores<-rbind(entity1score)
}
twtList1$text
library(twitteR)
library(wordcloud)
library(rCharts)
library(stringr)
library(RJSONIO)
library(plyr)
library(tm)
consumer_key <- 'lcbhmHyr2UmRTEghBDP1iiJcf'
consumer_secret <- '7eDC1tfIytttARfi2VWLf2XCFIBfPeS00KHByFpD0kJstYm8Ea'
access_token <- '2515844348-wtNF44w9eOYQoxEmHt0miAtTbVBpH7BSv55VnEN'
access_secret <- 'WFMrMZFaf0G1jeTQkNwVnORhYq0SLvFHq2Hwkz1uf9sAx'
origop <- options("httr_oauth_cache")
options(httr_oauth_cache=FALSE)
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
twList<-searchTwitter(searchString = "#GoT",lang = "en",n=50000)
twList
twList<-searchTwitter(searchString = "#GoT",lang = "en",n=50000)
twList
twtList1<- do.call("rbind",lapply(twList,as.data.frame))
twtList1$text<-iconv(twtList1$text, 'UTF-8', 'ASCII')
col(twtList1)
colnames(twtList1)
twtList1$latitude
myCorpus<-Corpus(VectorSource(twtList1$text))
myCorpus<-tm_map(myCorpus,tolower)
myCorpus<-tm_map(myCorpus,removePunctuation)
myCorpus<-tm_map(myCorpus,removeNumbers)
myCorpusCopy<-myCorpus
myCorpus<-tm_map(myCorpus,stemDocument)
myCorpus<-myCorpusCopy
tdm<-TermDocumentMatrix(myCorpus)
tweets<-as.character(twtList1$text)
myCorpus<-Corpus(VectorSource(tweets))
myCorpus<-tm_map(myCorpus,tolower)
myCorpus<-tm_map(myCorpus,removePunctuation)
myCorpus<-tm_map(myCorpus,removeNumbers)
tdm<-TermDocumentMatrix(myCorpus)
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(text.corpus,min.freq = 1, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(text.corpus,min.freq = 10, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 100)
wordcloud(text.corpus,min.freq = 30, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 100)
colnames(twtList1)
getUser(twtList1[1,11])
users<-getUser(twtList1$screenName)
users<-lookupUsers(twtList1$screenName)
dims(users)
dim(users)
users
users[1]
users[1]$getScreenNAme()
users[1]$getScreenName()
lookupUsers(users = c("chrislfc28"))
lookupUsers(users = c("chrislfc28"))
TermDocumentMatrix(text.corpus)
tdm<-TermDocumentMatrix(text.corpus)
freq<-findFreqTerms(tdm,lowfreq = 15)
freq
freq<-findFreqTerms(tdm,lowfreq = 50)
freq
freq<-findFreqTerms(tdm,lowfreq = 200)
freq
freq<-findFreqTerms(tdm,lowfreq = 250)
freq
freq<-findFreqTerms(tdm,lowfreq = 500)
freq
freq<-findFreqTerms(tdm,lowfreq = 100)
freq
term.frequency <- rowSums(as.matrix(tdm))
findAssocs(tdm,"sansa",0.2)
findAssocs(tdm,"tyrion",0.2)
library(graph)
install.packages('graph')
install.packages('Rgraphviz')
plot(tdm,term = freq,corThreshold = 0.12, weighting = T)
install.packages('topicmodels')
library(topicmodels)
drm<-as.DocumentTermMatrix(tdm)
lda<-LDA(drm,k=8)
findAssocs(tdm,"finale",0.2)
twList<-searchTwitter(searchString = "#gotfinale",lang = "en",n=500)
twList
library(twitteR)
library(wordcloud)
library(rCharts)
library(stringr)
library(RJSONIO)
library(plyr)
library(tm)
consumer_key <- 'lcbhmHyr2UmRTEghBDP1iiJcf'
consumer_secret <- '7eDC1tfIytttARfi2VWLf2XCFIBfPeS00KHByFpD0kJstYm8Ea'
access_token <- '2515844348-wtNF44w9eOYQoxEmHt0miAtTbVBpH7BSv55VnEN'
access_secret <- 'WFMrMZFaf0G1jeTQkNwVnORhYq0SLvFHq2Hwkz1uf9sAx'
origop <- options("httr_oauth_cache")
options(httr_oauth_cache=FALSE)
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
twList<-searchTwitter(searchString = "#fifa15 #gk",lang = "en",n=500)
twtList
twList
twList<-searchTwitter(searchString = "#fifa15 #bestgk",lang = "en",n=500)
twList<-searchTwitter(searchString = "#bestgk",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "#fifa15",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "#careermode",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "fifa15 gk",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "fifa15 best gk",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "fifa15 best gk",lang = "en",n=500)
twList<-searchTwitter(searchString = "fifa15 best cm",lang = "en",n=500)
twList
twList<-searchTwitter(searchString = "fifa15 best lm",lang = "en",n=500)
twList<-searchTwitter(searchString = "fifa15 lm",lang = "en",n=500)
twList
library(twitteR)
library(wordcloud)
library(rCharts)
library(stringr)
library(RJSONIO)
library(plyr)
library(tm)
consumer_key <- 'lcbhmHyr2UmRTEghBDP1iiJcf'
consumer_secret <- '7eDC1tfIytttARfi2VWLf2XCFIBfPeS00KHByFpD0kJstYm8Ea'
access_token <- '2515844348-wtNF44w9eOYQoxEmHt0miAtTbVBpH7BSv55VnEN'
access_secret <- 'WFMrMZFaf0G1jeTQkNwVnORhYq0SLvFHq2Hwkz1uf9sAx'
origop <- options("httr_oauth_cache")
options(httr_oauth_cache=FALSE)
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
twList<-searchTwitter(searchString = "fifa15 cm",lang="en",n=500)
twtList1<- do.call("rbind",lapply(twList,as.data.frame))
twtList1$text
twtList1$text<-iconv(twtList1$text, 'UTF-8', 'ASCII')
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(text.corpus,min.freq = 1, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud(text.corpus,min.freq = 20, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud(text.corpus,min.freq = 10, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud(text.corpus,min.freq = 10, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 50)
wordcloud(text.corpus,min.freq = 5, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 50)
wordcloud(text.corpus,min.freq = 5, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 100)
twtList1$text
twtList1$text<-gsub(twtList1$text,"\n","")
twtList1$text
twList<-searchTwitter(searchString = "fifa15 cm",lang="en",n=500)
twtList1<- do.call("rbind",lapply(twList,as.data.frame))
CleanTweets<-function(tweets)
{
# Remove redundant spaces
tweets <- str_replace_all(tweets," "," ")
# Get rid of URLs
tweets <- str_replace_all(tweets, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# Take out retweet header, there is only one
tweets <- str_replace(tweets,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweets <- str_replace_all(tweets,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweets <- str_replace_all(tweets,"@[a-z,A-Z]*","")
return(tweets)
}
twtList1$text
twtList1$text<-CleanTweets(twtList1$text)
twtList1$text
CleanTweets<-function(tweets)
{
# Remove redundant spaces
tweets <- str_replace_all(tweets," "," ")
# Get rid of URLs
tweets <- str_replace_all(tweets, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# Take out retweet header, there is only one
tweets <- str_replace(tweets,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweets <- str_replace_all(tweets,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweets <- str_replace_all(tweets,"@[a-z,A-Z]*","")
return(tweets)
}
CleanTweets<-function(tweets)
{
# Remove redundant spaces
tweets <- str_replace_all(tweets," "," ")
# Get rid of URLs
tweets <- str_replace_all(tweets, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# Take out retweet header, there is only one
tweets <- str_replace(tweets,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweets <- str_replace_all(tweets,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweets <- str_replace_all(tweets,"@[a-z,A-Z]*","")
tweets <- str_replace_all(tweets,"\n","")
return(tweets)
}
twtList1$text<-CleanTweets(twtList1$text)
twtList1$text
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(text.corpus,min.freq = 1, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud(text.corpus,min.freq = 5, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
twtList1$text<-iconv(twtList1$text, 'UTF-8', 'ASCII')
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
wordcloud(text.corpus,min.freq = 1, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 20)
wordcloud(text.corpus,min.freq = 5, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 50)
wordcloud(text.corpus,min.freq = 1, scale=c(8,0.3),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 50)
twtList1$text
twList<-searchTwitter(searchString = "fifa15 cm",lang="en",n=500)
twList<-searchTwitter(searchString = "fifa15 cm",lang="en",n=500)
twtList1<- do.call("rbind",lapply(twList,as.data.frame))
twtList1$text<-CleanTweets(twtList1$text)
twtList1$text
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
text.corpus
tdm<-TermDocumentMatrix(text.corpus)
findFreqTerms(tdm,lowfreq = 5)
findFreqTerms(tdm,lowfreq = 2)
findFreqTerms(tdm,lowfreq = 1)
findAssocs(tdm,"tielmans")
findAssocs(tdm,"tielmans",0.2)
findAssocs(tdm,"tielmans",00)
findAssocs(tdm,"tielmans",0.5)
findAssocs(tdm,"tielemans",0.5)
findAssocs(tdm,"tielemans",0.2)
findAssocs(tdm,"tshibola",0.2)
source("http://bioconductor.org/biocLite.R")
source("http://bioconductor.org/biocLite.R")
biocLite()
library(Rgraphviz)
biocLite("Rgraphviz")
freq = findFreqTerms(tdm,lowfreq = 2)
freq
library(graph)
library(Rgraphviz)
plot(tdm, term = freq, corThreshold = 0.12, weighting = T)
tdm
tdm<-TermDocumentMatrix(twtList1$text)
tdm<-TermDocumentMatrix(as.character(twtList1$text))
tdm<-TermDocumentMatrix(text.corpus)
text.corpus
twtList1$text<-iconv(twtList1$text, 'UTF-8', 'ASCII')
twtList1$text
wordcloud_base<-as.character(twtList1$text,na.omit = T)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
tdm<-TermDocumentMatrix(text.corpus)
freq = findFreqTerms(tdm, lowfreq = 2)
freq
freq = findFreqTerms(tdm, lowfreq = 1)
freq
plot(tdm,freq,corThreshold = 0.12, weighting = T)
plot(tdm,freq,corThreshold = 0.5, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 5)
freq
plot(tdm,freq,corThreshold = 0.5, weighting = T)
plot(tdm,freq,corThreshold = 0.12, weighting = T)
plot(tdm,freq,corThreshold = 0.1, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 2)
plot(tdm,freq,corThreshold = 0.1, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 10)
freq
plot(tdm,freq,corThreshold = 1.0, weighting = T)
plot(tdm,freq,corThreshold = 2.0, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 1)
plot(tdm,freq,corThreshold = 2.0, weighting = T)
plot(tdm,freq,corThreshold = 1.0, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 2)
plot(tdm,freq,corThreshold = 1.0, weighting = T)
plot(tdm,freq,corThreshold = 2.0, weighting = T)
freq = findFreqTerms(tdm, lowfreq = 5)
plot(tdm,freq,corThreshold = 1.0, weighting = T)
plot(tdm,freq,corThreshold = 0.1, weighting = T)
twList<-searchTwitter(searchString = "fifa15 cb",lang="en",n=500)
twtList1<- do.call("rbind",lapply(twList,as.data.frame)
)
twtList1$text
twtList1$text<-iconv(twtList1$text, 'UTF-8', 'ASCII')
twtList1$text
wordcloud_base<-as.character(twtList1$text, na.omit = T)
twList<-searchTwitter(searchString = "fifa15 cb",lang="en",n=500)
twtList1<- do.call("rbind",lapply(twList,as.data.frame))
twtList1$text<-CleanTweets(twtList1$text)
twtList1$text
wordcloud_base<-as.character(twtList1$text)
text<-sapply(wordcloud_base,function(x) x)
text.corpus<-Corpus(VectorSource(text))
text.corpus<-tm_map(text.corpus, function(x)removeWords(x,stopwords()))
tdm<-TermDocumentMatrix(text.corpus)
findFreqTerms(tdm,lowfreq = 5)
findFreqTerms(tdm,lowfreq = 2)
findFreqTerms(tdm,lowfreq = 1)
findFreqTerms(tdm,lowfreq = 2)
freq = findFreqTerms(tdm,lowfreq = 2)
plot(tdm,freq,corThreshold = 0.1, weighting = T)
plot(tdm,freq,corThreshold = 1.0, weighting = T)
plot(tdm,freq,corThreshold = 0.2, weighting = T)
freq = findFreqTerms(tdm,lowfreq = 10)
plot(tdm,freq,corThreshold = 0.2, weighting = T)
freq = findFreqTerms(tdm,lowfreq = 5)
plot(tdm,freq,corThreshold = 0.2, weighting = T)
freq = findFreqTerms(tdm,lowfreq = 3)
plot(tdm,freq,corThreshold = 0.2, weighting = T)
twtList1$text
wordcloud_base<-as.character(twtList1$text)
wordcloud_base
text<-sapply(wordcloud_base,function(x) x)
text
text.corpus<-Corpus(VectorSource(text))
tdm<-TermDocumentMatrix(tdm)
tdm<-TermDocumentMatrix(text.corpus)
freq = findFreqTerms(tdm,lowfreq = 2)
freq
freq = findFreqTerms(tdm,lowfreq = 5)
freq
freq = findFreqTerms(tdm,lowfreq = 3)
freq
findAssocs(tdm,"ashcroft5")
findAssocs(tdm,"ashcroft5"corlimit = 0.2)
findAssocs(tdm,"ashcroft5",corlimit = 0.2)
twtList1$text
findAssocs(tdm,"CUNNINGHAM",corlimit = 0.2)
findAssocs(tdm,"CUNNINGHAM_17",corlimit = 0.2)
findAssocs(tdm,"cunningham_17",corlimit = 0.2)
findAssocs(tdm,"browning",corlimit = 0.2)
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
devtools::install_github("jcheng5/googleCharts")
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data-science-coursera/developing data products/assignment')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data-science-coursera/developing data products/assignment')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp('Documents/data science-nanodegree/data visualisation/storm-dataset-explorer')
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
setwd("~/Documents/data science-nanodegree/data visualisation/storm-dataset-explorer")
dt <- fread('data/events.agg.csv')
tmp <- merge(
data.table(STATE=sort(unique(dt$STATE))),
dt[
YEAR >= 1950 & YEAR <= 2011,
list(
COUNT=sum(COUNT),
INJURIES=sum(INJURIES),
FATALITIES=sum(FATALITIES),
PROPDMG=round(sum(PROPDMG), 2),
CROPDMG=round(sum(CROPDMG), 2)
),
by=list(STATE)],
by=c('STATE'), all=TRUE
)
shiny::runApp()
install.packages('dplyr')
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
states<-unique(dt$STATE)
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
dt$EVTYPE
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
library(shinnyapps)
library(shinyapps)
deloyApp()
deployApp()
install.packages("BH")
deployApp()
deployApp()
shiny::runApp('~/Documents/data-science-coursera/developing data products/assignment')
setwd("~/Documents/data-science-coursera/developing data products/assignment")
deployApp()
setwd("~/Documents/data science-nanodegree/data visualisation/storm-dataset-explorer")
deployApp()
setwd("~/Documents/data science-nanodegree/ml/final_project")
x<-read.csv('data.csv')
